{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Python For Data Analysis\n",
    "## Class 3\n",
    "\n",
    "The objectives of this class are for y'all to have:\n",
    "\n",
    "1. Learned a few more useful `pandas` patterns\n",
    "2. Generated some plots with seaborn\n",
    "3. Done some more exploratory data analysis on your own\n",
    "4. Ran a linear regression (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # use 'as' keyword to namespace a package\n",
    "import numpy as np\n",
    "complaints = pd.read_csv('../pandas-cookbook/data/311-service-requests.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_cols = ['Created Date', \n",
    "               'Closed Date',\n",
    "               'Due Date', \n",
    "               'Agency',\n",
    "               'Facility Type',\n",
    "               'Agency Name', \n",
    "               'Complaint Type', \n",
    "               'Borough', \n",
    "               'Status', \n",
    "               'Descriptor']\n",
    "cleaned = complaints[useful_cols]\n",
    "cleaned = cleaned.rename(columns=lambda x: x.lower().replace(' ','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Week's Exercise\n",
    "\n",
    "Write a function that takes a column name, a number n, and a dataframe as an argument, and returns a column with the top n categories and all other categories as \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_n(col_name, top_n, df):\n",
    "    # Get the value counts of our column name from our DF\n",
    "    value_counts = df[col_name].value_counts()\n",
    "    if len(value_counts) <= top_n:\n",
    "        print(\"\"\"WARNING: There are fewer distinct categories in df[%s] than requested. \n",
    "              No $replacement performed\"\"\", col_name)\n",
    "        return df[col_name] # Do no replacement, return the column\n",
    "    keep_vals = list(value_counts.head(top_n).index) # Get the top_n rows from value_counts\n",
    "    keep_mask = df[col_name].isin(keep_vals) # Identify the \"keeper\" rows\n",
    "    new_x = df[col_name].copy() # Copy to prevent in-place editing\n",
    "    new_x[~keep_mask] = 'other' # Negate the mask to identify those we want to replace\n",
    "    replace_count = len(value_counts) - top_n\n",
    "    print(\"Replaced %s values\" % replace_count)\n",
    "    return(new_x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Creating a column with time-to-close\n",
    "# pd.options.mode.chained_assignment = None\n",
    "cleaned.dtypes\n",
    "cleaned['created_date'] = pd.to_datetime(cleaned['created_date'])\n",
    "cleaned['closed_date'] = pd.to_datetime(cleaned['closed_date'])\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned['time_to_resolution'] = (cleaned['closed_date'] - cleaned['created_date']) / np.timedelta64(1, 'm')\n",
    "cleaned.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group our data by complaint type\n",
    "by_complaint = cleaned.groupby('complaint_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average response time\n",
    "by_complaint['time_to_resolution'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###cleaned.loc[cleaned['time_to_resolution']<0,:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "  * What's going on with negative time-to-resolution?\n",
    "  * Determine which types of complaints are most often late (closed_date > due_date)\n",
    "      * Which types of complaints have the highest *percentage* of late calls?\n",
    "  * From which boroughs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More advanced Data Manipulations with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In pandas, the split-apply-combine pattern is one of the most powerful but least understood features of the tool. In fact, I don't even understand it very well, but we'll struggle through it together.\n",
    "\n",
    "We'll cover a few operations *in brief* with specific emphasis on\n",
    "* Indexes in pandas\n",
    "* groupby objects\n",
    "* unstack\n",
    "* pivot_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexes in pandas\n",
    "Indexes are convenient ways to keep track of the *grain* (i.e., what defines a \"row\") in a dataframe. Dataframes have the ability to have multiple indexes which allow for slicing-and-dicing in very sophisticated ways. Unfortunately this can also means there's a lot of complexity which can be overwhelming for people who are new to the framework.\n",
    "\n",
    "The thing to keep in mind is that indexes are **not** columns just like any other. They must be accessed (and manged) differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby\n",
    "\n",
    "As we saw above, we can use `groupby()` to summarize our data. The object returned by `groupby()` is not a dataframe -- in fact, it's more like 'instructions for grouping' than actual grouped data.\n",
    "\n",
    "```python\n",
    "grpd = cleaned.groupby('Status')\n",
    "# <pandas.core.groupby.DataFrameGroupBy object at 0x113aeada0>\n",
    "```\n",
    "\n",
    "Only when we apply some sort of function to perform an aggregation do we actually get results back\n",
    "\n",
    "```python\n",
    "grpd['Status'].count()\n",
    "\n",
    "Status\n",
    "Assigned       6189\n",
    "Closed        57165\n",
    "Email Sent      129\n",
    "Open          43972\n",
    "Pending        3165\n",
    "Started         447\n",
    "Unassigned        2\n",
    "Name: Status, dtype: int64\n",
    "```\n",
    "\n",
    "When we group-by data, the column we're grouping by becomes the index of the object we're returning (rather than a column of a table. Because we're now working explicitly with indexes (and sometimes multiple indexes!) it'll be helpful to look at some of the index-specific methods available to us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unstack\n",
    "\n",
    "If we group by multiple columns, we'll get data back with multiple indexes. We can \"unstack\" these indexes to get more tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_s = cleaned.groupby(['borough','status'])['status'].count()\n",
    "print(b_s.head(20))\n",
    "print(\"----------------------------\")\n",
    "print(\"Now Unstack!\")\n",
    "print(\"----------------------------\")\n",
    "print(b_s.unstack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"unstacked\" object now looks like tabular data that are much easier to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot Table\n",
    "\n",
    "\"Pivot tables\" are a powerful tool very common in the world of spreadsheet-first data analytics. In fact, when analysts are first making the move from excel to python or R pivot tables are the feature they miss the most (and they generally find the in-code approximations of these tools overly burdensome). Pandas, nicely, has an API that feels familiar to this flavor of analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flights_by_carrier = data.pivot_table(index='flight_date', columns='unique_carrier', values='flight_num', aggfunc='count')\n",
    "flights_by_carrier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_by_borough = cleaned.pivot_table(index=\"status\", columns=\"borough\", values=\"created_date\", aggfunc=\"count\")\n",
    "status_by_borough.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: \n",
    "* plot a line chart with complaints by day by borough (time on the x axis, one line per borough)\n",
    "\n",
    "```python\n",
    "# complaints[['Unique Key', 'Borough']].groupby([complaints.index.date, 'Borough']).count().unstack().plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Plotting with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do this part interactively\n",
    "bk_manh = cleaned.loc[cleaned['borough'].isin(['BROOKLYN', 'MANHATTAN'])]\n",
    "x = top_n('agency', 3, bk_manh).copy()\n",
    "bk_manh.loc[:,'cleaned_agency'] = x\n",
    "bk_manh = bk_manh.loc[bk_manh['time_to_resolution'] >= 0]\n",
    "bk_manh = bk_manh.loc[bk_manh['time_to_resolution'] <= 1000]\n",
    "\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(x=\"cleaned_agency\", y=\"time_to_resolution\", hue=\"borough\", data=bk_manh, split=True,\n",
    "               inner=\"quart\", palette={\"BROOKLYN\": \"b\", \"MANHATTAN\": \"y\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.violinplot(x=\"cleaned_agency\", y=\"time_to_resolution\", hue=\"borough\", data=bk_manh, split=True,\n",
    "               inner=\"quart\", palette={\"BROOKLYN\": \"b\", \"MANHATTAN\": \"y\"})\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Navigate to http://seaborn.pydata.org/tutorial/categorical.html and try out some of the categorical \n",
    "# plotting options with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(cleaned['time_to_resolution'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_zeroes = bk_manh.loc[bk_manh['time_to_resolution'] > 0]\n",
    "sns.distplot(no_zeroes['time_to_resolution'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(no_zeroes, row='borough', col='cleaned_agency', margin_titles=True)\n",
    "g.map(sns.distplot, \"time_to_resolution\")\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the coordinates of times square\n",
    "# 40.7589° N, 73.9851° W\n",
    "# Append column to df with distance to times square\n",
    "# sqrt((x_2 - x_1)**2 + (y_2 - y_1)**2)\n",
    "# Make a scatter plot\n",
    "# Introduce simple linear regression\n",
    "# Give intuitive explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
